{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Run Log Analysis and Visualization for AWS DeepRacer\n",
    "\n",
    "This notebook walks through how you can analyze and debug using the AWS DeepRacer Simulation logs \n",
    "\n",
    "```\n",
    "1. Tools to find best iteration of your model\n",
    "2. Visualize reward distribution on the track\n",
    "  2.1 Visualize reward heatmap per episode or iteration\n",
    "3. Identify hotspots on the track for your model\n",
    "4. Understand probability distributions on simulated images\n",
    "5. Evaluation run analysis - plot lap speed heatmap\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "boto3 >= 1.9.133  ; configure your aws cli and/or boto credentials file\n",
    "\n",
    "AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\n",
    "\n",
    "Boto Configuration: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.210'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "#Shapely Library\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.geometry.polygon import LinearRing, LineString\n",
    "\n",
    "import track_utils as tu\n",
    "import log_analysis as la\n",
    "import cw_utils as cw\n",
    "\n",
    "# Make sure your boto version is >= '1.9.133'\n",
    "cw.boto3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'track_utils' from '/home/ccsantos/deepracer-for-dummies/aws-deepracer-workshops/log-analysis/track_utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload log_analysis and the rest of stuff here if needed\n",
    "# (handy for code updates in utils, doesn't break anything if no changes)\n",
    "import importlib\n",
    "importlib.reload(la)\n",
    "importlib.reload(cw)\n",
    "importlib.reload(tu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load waypoints for the track you want to run analysis on\n",
    "\n",
    "Tracks Available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_track.npy\t  London_Loop_Train.npy    reinvent_base.npy\r\n",
      "Bowtie_track.npy  New_York_Eval_Track.npy  Straight_track.npy\r\n",
      "China_track.npy   New_York_Track.npy\t   Tokyo_Training_track.npy\r\n",
      "H_track.npy\t  Oval_track.npy\t   Virtual_May19_Train_track.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls tracks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71 waypoints\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"0.13165101625431158 0.09529031240092306 7.739630843825427 5.0005055286139175\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,5.191086153415764)\"><path fill-rule=\"evenodd\" fill=\"#ff3333\" stroke=\"#555555\" stroke-width=\"0.15479261687650855\" opacity=\"0.6\" d=\"M 2.9101200062595103,0.38230620783067915 L 3.320119934919921,0.38247723188186483 L 3.420119917520022,0.3825189450650809 L 3.630128223616876,0.3826065462298145 L 4.190119783540795,0.3828401365758443 L 4.50012807223775,0.382969450923794 L 4.550119720901156,0.38299030403542206 L 5.3201279295585735,0.3833114990261654 L 5.420127912158674,0.38335321220938146 L 5.780127849519036,0.38350337966895914 L 6.324972631375165,0.3956832604952143 L 6.496120858284972,0.4160202072156111 L 6.62472364224327,0.44473185107344176 L 6.815966218031551,0.521642101758542 L 7.017519684834686,0.6415111643040204 L 7.229499454034008,0.8217965373170768 L 7.354308823693938,0.9825323713128955 L 7.447931440559386,1.1453875219327774 L 7.584628865863982,1.8031173114342751 L 7.577929620223677,1.8535557187007277 L 7.571403728787527,1.90268894767995 L 7.350201669529235,2.492043640838347 L 7.258218658533806,2.610503855324811 L 6.843319429832621,2.9366420720078783 L 6.652076861387874,3.0135523418410606 L 6.077102317373509,3.0843225367722926 L 5.917114336078783,3.0829358686812904 L 5.717129359460375,3.081202533567537 L 5.667306448817149,3.080770702127258 L 5.197341753763893,3.076697364609938 L 5.047179687788711,3.0753958609364638 L 5.034886471281728,3.0759817113964028 L 4.97556588304942,3.082330184103404 L 4.723655995002148,3.1605758734283604 L 4.498355317486535,3.3624310927965775 L 4.3325507156607355,3.5658507056549404 L 4.240944136947374,3.6782394727455503 L 4.0138222049015155,3.9568870795046953 L 3.922215626188155,4.069275846595304 L 3.7888808345557994,4.232859434720925 L 3.4380291650003363,4.595552001126502 L 3.353319122878281,4.64793530145161 L 3.1961735515503995,4.727704914445076 L 3.055651529170767,4.776192024941823 L 2.840968530687481,4.80720241399628 L 2.84303014895641,4.80698759310109 L 2.4913683324088596,4.809142846799084 L 2.2404230806588203,4.8019019048214995 L 1.981215987743099,4.794422570158293 L 1.7305590426333723,4.787189947173157 L 1.1133976663098506,4.68342738185746 L 0.9132873376602105,4.575866464388443 L 0.4396815695265954,3.8869828266056468 L 0.4183040104700681,3.4707944950583007 L 0.5817439759375306,2.665552102434232 L 0.5930735980115257,2.6097328787804766 L 0.6242638438593316,2.4560635597666995 L 0.6437866949569175,2.3598776159940114 L 0.7253233176311822,1.9581598051701847 L 0.7467666116515489,1.8525121483482125 L 0.7963216533547308,1.6083624372691325 L 0.9415723136978997,1.058316414523293 L 0.962303954307788,1.006632340237715 L 1.0626459793805891,0.8300487672594938 L 1.095566151040184,0.7926169730979473 L 1.1657678278926826,0.712794342009927 L 1.3114141412298634,0.5840889274264379 L 1.3546711732192105,0.5599177517631669 L 2.040128500275279,0.3819433066166796 L 2.7501283767359923,0.38223947021751337 L 2.9101200062595103,0.38230620783067915 L 2.909906444083587,0.9840787499610828 L 2.749912415761146,0.9840310918882519 L 2.0399124787587044,0.9838196013153698 L 1.6445917267628223,1.0788141361524557 L 1.6322328186035158,1.0857186889648438 L 1.613667277893416,1.1068169893088786 L 1.5434438985939147,1.186620425727402 L 1.5296887207031251,1.2022521209716797 L 1.5185744153105296,1.2299234905909608 L 1.497822943058028,1.281588586929395 L 1.3902788553560892,1.7314629754755075 L 1.338638685255012,1.9753504435221483 L 1.3163469413409712,2.080630441068597 L 1.2314242341915527,2.4817054953489857 L 1.211040078964814,2.5779762860297337 L 1.178615724180754,2.7311108279154777 L 1.1667667487821711,2.787071460950787 L 0.9966111768378945,3.59068638968341 L 1.0248472113946108,3.7704349524329484 L 1.2783204962571983,4.088884910007067 L 1.314613037109375,4.11409423828125 L 1.7485951938134034,4.166053115531388 L 1.9992580924048617,4.173382658074824 L 2.258451665350191,4.180961642960754 L 2.509406720502155,4.1882997283477925 L 2.7808141160514976,4.192424015198027 L 2.7788712303071925,4.192614571024957 L 2.8843350219726562,4.182270812988281 L 2.994786471026911,4.144147305563019 L 3.028917153889106,4.123471202342913 L 3.09720703125,4.081221618652344 L 3.30923654574253,3.8419079701360612 L 3.442571258568735,3.6783243301144166 L 3.534177795123867,3.5659355126652286 L 3.7612995949291355,3.287287815055975 L 3.8529061314842683,3.1748989976067854 L 4.018710641305418,2.9714793128627117 L 4.399026282932134,2.6360679163260246 L 4.909864648480311,2.468324424054731 L 4.9691856595622115,2.46197790460427 L 5.018619835463795,2.4566891383955527 L 5.20260337467398,2.4551229239531156 L 5.6725712484683495,2.45900857506364 L 5.722402483931133,2.459420575130478 L 5.922388813205334,2.4610740436881477 L 6.082377876624694,2.462396818534284 L 6.431444702148438,2.436733856201172 L 6.6006591796875,2.3618621826171875 L 6.769062197464956,2.2227494158821073 L 6.863466114123764,2.106320230004854 L 6.961888826195919,1.8144451569358484 L 6.969601773690491,1.7656646935058404 L 6.972432250976563,1.7477633666992187 L 6.883023391314907,1.3793610957984892 L 6.865130542696974,1.345683363233008 L 6.767670288085938,1.2075256347656251 L 6.65414056181079,1.121153136506339 L 6.58552837663102,1.0751901389624254 L 6.433070068359375,1.0093801879882813 L 6.42648361684787,1.0086153150902388 L 6.255308201425604,0.9887370185843034 L 5.779912146912412,0.9849336502485796 L 5.419912178854835,0.984826415591907 L 5.319912187727731,0.9847966281872758 L 4.5499062985681,0.9845672633970358 L 4.499912260485474,0.9845523714692992 L 4.189906330510524,0.9844600287403632 L 3.6299123376796656,0.9842932210490071 L 3.4199063988318197,0.9842306657247023 L 3.319906407704715,0.9842008783200711 L 2.909906444083587,0.9840787499610828 L 2.9101200062595103,0.38230620783067915 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x7fd448bbb978>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_center_line, l_inner_border, l_outer_border, road_poly = tu.load_track(\"reinvent_base\")\n",
    "\n",
    "road_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all evaluation data\n",
    "\n",
    "It's good to keep things organised: group your files into folders to not lose track where they came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepracer-fe179db0-c1f3-11e9-8c5c-0242ac120004.log   :   1566166485.1706324\n",
      "c02f1706-c13c-11e9-8ad0-0242ac120004   :   1566080238.9316764\n",
      "deepracer-5bf07a28-c1ff-11e9-ae47-0242ac120004.log   :   1566166072.0296586\n",
      "deepracer-sim-ghy8fskj27j4.log   :   1566869693.3460143\n",
      "deepracer-Oval_track.log   :   1566167056.440198\n",
      "deepracer-Oval_Track.log   :   1566178685.2514682\n",
      "log   :   1566872798.891943\n",
      "deepracer-eval-sim-j5gdq7sxh2c2.log   :   1566872764.1999426\n",
      "deepracer-sim-2zfqgg08b2bl.log   :   1566260306.61707\n",
      "deepracer-sim-sample.log   :   1566167020.652166\n",
      "deepracer-dr-sm-rltj--20190819134949-f350b748-9893-4350-8d32-3869ab5038e3.log   :   1566261861.6997128\n",
      "deepracer-6ebf6bca-c13f-11e9-bd3a-0242ac120004.log   :   1566166471.5945964\n",
      "deepracer-sim-j5gdq7sxh2c2.log   :   1566261926.8624902\n",
      "\n",
      "Most recent file  = logs/log\n"
     ]
    }
   ],
   "source": [
    "#print log files and show most recent. You may want to use that file for analysis\n",
    "import os\n",
    "file_list = []\n",
    "for file in os.listdir(\"logs\"):\n",
    "    if(file==\"latest\"):\n",
    "        continue\n",
    "    file_list.append([os.stat(os.path.join(\"logs\", file)).st_mtime, os.path.join(\"logs\", file)])\n",
    "    file_list.sort(key=lambda x: x[0])  # sort by creation date\n",
    "    print(file + \"   :   \" + str(os.stat(os.path.join(\"logs\", file)).st_mtime))\n",
    "print(\"\\nMost recent file  = \" + file_list[-1][1])\n",
    "fname = file_list[-1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the desired log file given the simulation ID \n",
    "\n",
    "\n",
    "If you wish to bulk export the logs from Amazon Cloudwatch to Amazon S3 :: \n",
    "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasks.html\n",
    "\n",
    "_Use `force=True` to download again_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file exists, use force=True to download again\n",
      "SIM_TRACE_LOG:19,213,5.3787,1.0963,0.4406,0.52,0.50,8,0.0010,False,False,18.9704,8,17.67,1566861623.0788045\n",
      "SIM_TRACE_LOG:19,214,5.3894,1.1003,0.4347,0.00,0.50,4,0.0000,True,False,19.0314,8,17.67,1566861623.1478322\n",
      "Training> Name=main_level/agent, Worker=0, Episode=20, Total reward=217.23, Steps=2133, Training iteration=0\n"
     ]
    }
   ],
   "source": [
    "AWS_train_ID = 'sim-ghy8fskj27j4'\n",
    "stream_name = AWS_train_ID ## CHANGE This to your simulation application ID\n",
    "fname = 'logs/deepracer-%s.log' %stream_name\n",
    "cw.download_log(fname, stream_prefix=stream_name)\n",
    "\n",
    "!tail -n 3 $fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trace training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES_PER_ITERATION = 20 #  Set to value of your hyperparameter in training\n",
    "\n",
    "data = la.load_data(fname)\n",
    "#df = la.convert_to_pandas(data, episodes_per_iteration=EPISODES_PER_ITERATION)\n",
    "df = la.convert_to_pandas(data)\n",
    "\n",
    "df = df.sort_values(['episode', 'steps'])\n",
    "# personally I think normalizing can mask too high rewards so I am commenting it out,\n",
    "# but you might want it.\n",
    "# la.normalize_rewards(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the first line below: it takes a reward class from log-analysis/rewards, imports, instantiates and recalculates for the log. This lets you do some testing before you start training and rule out some obvious things.\n",
    "\n",
    "Just remember: not all params are provided, you are free to implement them and raise a Pull Request for log_analysis.df_to_params method.\n",
    "\n",
    "Wrap your reward function like in the sample one below.\n",
    "\n",
    "Final warning: there is a loss of precision in the logs and also potentially potential bugs. If you find any, please fix, please report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'log_analysis' has no attribute 'new_reward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fff83e1269b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_center_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reward.reward_sample'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, verbose=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msimulation_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation_agg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_training_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_agg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training progress'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'log_analysis' has no attribute 'new_reward'"
     ]
    }
   ],
   "source": [
    "la.new_reward(df, l_center_line, 'reward.reward_sample') #, verbose=True)\n",
    "\n",
    "simulation_agg = la.simulation_agg(df)\n",
    "\n",
    "la.analyze_training_progress(simulation_agg, title='Training progress')\n",
    "\n",
    "# This gives the warning about ptp method deprecation. The code looks as if np.ptp was used, I don't know how to fix it.\n",
    "la.scatter_aggregates(simulation_agg, 'Stats for all laps')\n",
    "\n",
    "complete_ones = simulation_agg[simulation_agg['progress']==100]\n",
    "\n",
    "if complete_ones.shape[0] > 0:\n",
    "    la.scatter_aggregates(complete_ones, 'Stats for complete laps')\n",
    "\n",
    "la.analyze_categories(simulation_agg, title='Quintiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calls below are useful when you want to look at some values largest or smallest. You can then take the episode number and scatter it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is useful if you struggle to get a complete lap\n",
    "simulation_agg.nlargest(10, 'new_reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastest complete laps\n",
    "complete_ones.nsmallest(5, 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best rewards in completed laps\n",
    "complete_ones.nlargest(5, 'reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best rewards in completed laps\n",
    "complete_ones.nlargest(5, 'new_reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is useful if you struggle to get a complete lap\n",
    "simulation_agg.nlargest(5, 'progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list all entries aggregated per episode. Handy for comparing the outcomes for reward and new_reward,\n",
    "# but for not much else.\n",
    "simulation_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# list all parsed log lines. Handy for comparing the outcomes for reward and new_reward,\n",
    "# but for not much else.\n",
    "df[df['episode']==10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This shows a histogram of actions per waypoint. Will let you spot potentially problematic places\n",
    "episode = df[df['episode']==771]\n",
    "episode[:-1].plot.bar(x='closest_waypoint', y='reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the reward distribution for your reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path taken for top reward iterations\n",
    "\n",
    "NOTE: in a single episode, the car can go around multiple laps, the episode is terminated when car completes 1000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "action_map, episode_map, sorted_idx = la.episode_parser(data)    \n",
    "fig = la.plot_top_laps(sorted_idx[:],  episode_map, l_center_line, l_inner_border, l_outer_border, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because Kumo Torakku has negative y values, I shamelessly took\n",
    "# RichardFan's modificationg for plot_track and refactored it to offer an x_shift and y_shift\n",
    "# They may not apply to other tracks. You will need to change it in the future. Simply add parameters:\n",
    "# track_size=(700,1000), y_shift=300\n",
    "track = la.plot_track(df, l_center_line, l_inner_border, l_outer_border)\n",
    "\n",
    "plt.title(\"Reward distribution for all actions \")\n",
    "im = plt.imshow(track, cmap='hot', interpolation='bilinear', origin=\"lower\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a particular iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_id = 3\n",
    "track = la.plot_track(df[df['iteration'] == iteration_id], l_center_line, l_inner_border, l_outer_border)\n",
    "plt.title(\"Reward distribution for all actions \")\n",
    "im = plt.imshow(track, cmap='hot', interpolation='bilinear', origin=\"lower\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path taken in a particular episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation RUN\n",
    "def plot_episode_run(df, E, center_line, inner_border, outer_border):\n",
    "    fig = plt.figure(1, figsize=(12, 16))\n",
    "    ax = fig.add_subplot(211)\n",
    "    la.print_border(ax, center_line, inner_border, outer_border) \n",
    "    episode_data = df[df['episode'] == E]\n",
    "    for row in episode_data.iterrows():\n",
    "        x1,y1,action,reward = row[1]['x'], row[1]['y'], row[1]['action'], row[1]['reward']\n",
    "        car_x2, car_y2 = x1 - 0.02, y1\n",
    "        plt.plot([x1, car_x2], [y1, car_y2], 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episode_run(df, 122, l_center_line, l_inner_border, l_outer_border) # arbitrary episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path taken in a particular Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iteration_id = 10\n",
    "EPISODE_PER_ITER = 40 # TODO this can be fetched from training logs\n",
    "\n",
    "for i in range((iteration_id-1)*EPISODE_PER_ITER, (iteration_id)*EPISODE_PER_ITER):\n",
    "    plot_episode_run(df, i, l_center_line, l_inner_border, l_outer_border)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk training load\n",
    "\n",
    "This is some slow and heavy stuff in here. You can download all logs (or part of them if you play with `not_older_than` and `older_than` parameters that take a string representation of a date in ISO format, for instance `DD-MM-YYYY` works).\n",
    "\n",
    "Since it can be a lot of downloading, it is commented out in here.\n",
    "\n",
    "Files downloaded once will not be downloaded again unless you add `force=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logs = cw.download_all_logs('logs/SELECT_YOUR_SUBFOLDER/training/deepracer-', '/aws/robomaker/SimulationJobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load every log from a folder. Every single one. This is a lot of data. If you want to save yourself some time later, below you have code to save and load all that with use of pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_folder = 'logs/SELECT_YOUR_SUBFOLDER/training'\n",
    "df_list = list()\n",
    "big_training_panda = None\n",
    "for stream in os.listdir(base_folder):\n",
    "    data = la.load_data('%s/%s' % (base_folder, stream))\n",
    "    df = la.convert_to_pandas(data)\n",
    "    df['stream'] = stream[10:]\n",
    "    if big_training_panda is not None:\n",
    "        big_training_panda = big_training_panda.append(df)\n",
    "    else:\n",
    "        big_training_panda = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have I mentioned a lot of data? This stores the data preprocessed for time savings\n",
    "big_training_panda.to_pickle('bulk_training_set.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_pickle\n",
    "\n",
    "big_training_panda = read_pickle('bulk_training_set.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as usual, handle with care. Towards the end of the London Loop I needed 30-45 minutes to recalculate the reward.\n",
    "#la.new_reward(big_training_panda, l_center_line, 'reward.reward_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "big_simulation_agg = la.simulation_agg(big_training_panda, 'stream')\n",
    "\n",
    "big_complete_ones = big_simulation_agg[big_simulation_agg['progress']==100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = big_simulation_agg.groupby(['stream'])\n",
    "\n",
    "for name, group in grouped:\n",
    "    la.scatter_aggregates(group, title=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of London Loop I had so much noise and random tries that wanted to find the most promising version of my model to submit. I used the below piece of code to iterate through all the stream values to detect the one with most promising times histogram. I should've added progress as well since the fastest ones hardly ever completed a lap. I will leave adding that as an exercise for the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values = []\n",
    "show = []\n",
    "show_above = -1\n",
    "i = 0\n",
    "for value in big_complete_ones.stream.values:\n",
    "    if value in values:\n",
    "        continue\n",
    "    values.append(value)\n",
    "    if i in show or i > show_above:\n",
    "        print(value)\n",
    "        big_complete_ones[big_complete_ones['stream']==value].hist(column=['time'], bins=20)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display loads of everything\n",
    "big_simulation_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action breakdown per iteration and historgram for action distribution for each of the turns - reinvent track\n",
    "\n",
    "This plot is useful to understand the actions that the model takes for any given iteration. \n",
    "\n",
    "This is a bit of an attempt to abstract away from the brilliant function in the original notebook towards a more general graph that we could use. It should be treated as a work in progress. The track_breakdown could be used as a starting point for a general track information object to handle all the customisations needed in methods of this notebook.\n",
    "\n",
    "A breakdown track data needs to be available for it. If you cannot find it for the desired track, MAKEIT.\n",
    "\n",
    "Currently supported tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.track_breakdown.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second parameter is either a single index or a list of indices for df iterations that you would like to view. You can for instance use `sorted_idx` list which is a sorted list of iterations from the highest to lowest reward.\n",
    "\n",
    "Bear in mind that you will have to provide a proper action naming in parameter `action_names`, this function assumes only six actions by default. I think they need to match numbering of actions in your model's metadata json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "la.action_breakdown(df, 20, la.track_breakdown['reinvent2018'], l_center_line, l_inner_border, l_outer_border)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log-analysis.venv",
   "language": "python",
   "name": "log-analysis.venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
